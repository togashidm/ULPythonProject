{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4055 Project Group 7\n",
    "\n",
    "# Notebook 2 - Predictive Modelling\n",
    "\n",
    "__Note:__ This notebook may take 5-10 minutes to run \n",
    "\n",
    "===================================================================================================================\n",
    "\n",
    "# 1 - Imports / Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVR, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv('./studentPor.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Functions Used in Notebook:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for extracting the names of features that are selected during the feature selection process\n",
    "def get_feature_names(feature_selector, dataframe, target):\n",
    "    feature_names = dataframe.drop(target, axis=1).columns\n",
    "    selected_features = feature_names[feature_selector.get_support()].tolist()\n",
    "    return selected_features\n",
    "\n",
    "# for plotting a confusion matrix as a heatmap\n",
    "def plot_confusion_matrix_heatmap(true_values, pred_values):\n",
    "    pred_values = pd.Series(pred_values)\n",
    "    array = confusion_matrix(true_values, pred_values)\n",
    "    df_cm = pd.DataFrame(array, index = [i for i in \"ABCDEF\"], columns = [i for i in \"ABCDEF\"])\n",
    "    plt.figure(figsize = (5,4))\n",
    "    ax = sns.heatmap(df_cm, annot=True, annot_kws={\"size\": 13}, cmap=\"Reds\", fmt='g', cbar=False)\n",
    "    ax.xaxis.tick_top()\n",
    "    ax.xaxis.set_label_position('top')\n",
    "    ax.tick_params(length=0)\n",
    "    plt.yticks(rotation=0)\n",
    "    bottom, top = ax.get_ylim()\n",
    "    ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('True Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================\n",
    "\n",
    "# 2 - Remove Outliers\n",
    "\n",
    "Refer to the EDA_FINAL_DOC Notebook for a detailed explanation of the removal of outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1 = df['absences'].quantile(0.25)\n",
    "Q3 = df['absences'].quantile(0.75)\n",
    "IQR = Q3 - Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['iqr'] = (df['absences'] < (Q1 - 1.5 * IQR)) | (df['absences'] > (Q3 + 1.5 * IQR))\n",
    "\n",
    "# a new df with outliers removed\n",
    "df_data = df[df.iqr != True]\n",
    "df_data = df_data.drop('iqr', axis = 1)\n",
    "\n",
    "# reset the dataframe index\n",
    "df_data = df_data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================\n",
    "\n",
    "# 3 - Convert Categorical Attributes to Numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dummies = pd.get_dummies(df_data, drop_first=True, columns=['school',\n",
    "                                                               'sex',\n",
    "                                                               'address',\n",
    "                                                               'famsize',\n",
    "                                                               'Pstatus',\n",
    "                                                               'Mjob',\n",
    "                                                               'Fjob',\n",
    "                                                               'reason',\n",
    "                                                               'guardian',\n",
    "                                                               'schoolsup',\n",
    "                                                               'famsup',\n",
    "                                                               'paid',\n",
    "                                                               'activities',\n",
    "                                                               'nursery',\n",
    "                                                               'higher',\n",
    "                                                               'internet',\n",
    "                                                               'romantic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have artificially increased the number of features from 33 to 41 in this step.\n",
    "\n",
    "This is beacuse the get_dummies method adds extra features when converting categorical features to numeric.\n",
    "\n",
    "__For example:__\n",
    "\n",
    "If a feature has 3 categories (like __guardian__ - mother, father and other), then the method adds 2 extra features:\n",
    "1. __guardian_mother__ - which will have a value of 0 or 1\n",
    "2. __guradian_other__ - which will have a value of 0 or 1\n",
    "\n",
    "Only one of these features can have a value of 1, and if both are 0 then it means that the guardian is the father by default. The method then deletes the original guardian column with the 2 mentioned here replacing it.\n",
    "\n",
    "===================================================================================================================\n",
    "\n",
    "# 4 - Reduce Target to Fewer Categories\n",
    "\n",
    "Beacuse we have 20 categories to which our target feature may belong, it would be very difficult to train a predictive model to correctly predict 1 of these 20 categories.\n",
    "\n",
    "Our initial instinct was to overcome this by reducing the target feature categories into binary values (pass/fail or 0/1).\n",
    "\n",
    "We explored this and quickly realised this caused imbalance in the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pass    599\n",
       "fail     29\n",
       "Name: grade, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the actual values for G3 and save to a numpy array\n",
    "G3 = df_dummies[\"G3\"].values\n",
    "grade = []\n",
    "# iterate through every G3 value and save the corresponding grade category to the grade array\n",
    "for number in G3:\n",
    "    if number >= 8:\n",
    "        grade.append('pass') # pass\n",
    "    else:\n",
    "        grade.append('fail') # fail\n",
    "\n",
    "# convert the grade array to a pandas dataframe\n",
    "grade = pd.DataFrame(data=grade, columns=[\"grade\"])\n",
    "grade['grade'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that there are far more passes than fails. This imbalance may skew our predictive models and result in a poor performance.\n",
    "\n",
    "We then decided a better approach would be to use the traditional 6 grade categories of A-F.\n",
    "\n",
    "This will give us a better chance of predicting the target as we have only 6 categories instead of 20.\n",
    "\n",
    "This also provides a relatively more balanced target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    172\n",
       "4    156\n",
       "2    141\n",
       "1     84\n",
       "0     46\n",
       "5     29\n",
       "Name: grade, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grade = []\n",
    "# iterate through every G3 value and save the corresponding grade category to the grade array\n",
    "for number in G3:\n",
    "    if number >= 17:\n",
    "        grade.append('0') # grade = A\n",
    "    elif number >= 15:\n",
    "        grade.append('1') # grade = B\n",
    "    elif number >= 13:\n",
    "        grade.append('2') # grade = C\n",
    "    elif number >= 11:\n",
    "        grade.append('3') # grade = D\n",
    "    elif number >= 8:\n",
    "        grade.append('4') # grade = E\n",
    "    else:\n",
    "        grade.append('5') # grade = F\n",
    "\n",
    "# convert the grade array to a pandas dataframe\n",
    "grade = pd.DataFrame(data=grade, columns=[\"grade\"])\n",
    "# create a new dataframe called data by joining grade with df_dummies\n",
    "data = pd.concat([df_dummies, grade], axis=1)\n",
    "# remove the original numeric G3 column because it is not needed\n",
    "data.drop(columns=\"G3\", axis=1, inplace=True)\n",
    "grade['grade'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================\n",
    "\n",
    "# 5 - Declare Predictors and Target\n",
    "\n",
    "We identify our target feature and our predictor features for our predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target attribute\n",
    "target_attribute_name = \"grade\"\n",
    "target = data[target_attribute_name]\n",
    "\n",
    "# predictor attributes\n",
    "predictors = data.drop(target_attribute_name, axis = 1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================\n",
    "\n",
    "# 6 - Split Dataset into Training and Test\n",
    "\n",
    "Before building a predictive model, we first want to split the entire dataset up into a training dataset and a test dataset.\n",
    "\n",
    "We will use the training dataset to build multiple predictive models. We then pick the best performing model and fit this to our test dataset in order to get the overall final accuracy of the model.\n",
    "\n",
    "The ratio of training to test data is usually varied from 60-80% training data and 20-40% test data. For a smaller dataset it is recommened to use 80% for training and 20% for testing. This is the ratio we will use.\n",
    "\n",
    "In order to maintain the same ratio of target categories in both datasets, it is neccessary to stratify the dataset.\n",
    "\n",
    "We also assign the random_state a constant value (seed). This is to ensure an identical split each time the program runs. This seed value is also assigned to random_state in some of our algorithms in section 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign a constant value to be used for random_state (otherwise a random number is chosen)\n",
    "seed = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare independent stratified data sets for training and testing of the models\n",
    "predictors_train, predictors_test, target_train, target_test = train_test_split(predictors, \n",
    "                                                                                target, \n",
    "                                                                                test_size=0.20, \n",
    "                                                                                shuffle=True, \n",
    "                                                                                stratify=target,\n",
    "                                                                                random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================\n",
    "\n",
    "# 7 - Feature Selection\n",
    "\n",
    "We will use the same two feature selection methods (estimators) that were used in lab 5. These include __SVR__ (Support Vector Regression) and __LR__ (Logistic Regression).\n",
    "\n",
    "Each estimator will be fitted to the training dataset in order to predict the target. This will happen in conjuction with __Recursive Feature Elimantion (RFE)__. RFE will run recursively, each time picking different subsets of features to train. The poorest performing features will be eliminated and the best subset of features will be returned at the end.\n",
    "\n",
    "__K-fold Cross Validation__ is also incorporated in the training process. We have chosen to use __5 folds__ here as the dataset is relatively small.\n",
    "\n",
    "This feature selection process is performed only once. The selected features in section 7 are then passed into each algorithm in section 8. There is no need to perform this step again for each algorithm as the resulting selected features will be the same each time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Using Support Vector Regression (SVR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['failures',\n",
       " 'G1',\n",
       " 'G2',\n",
       " 'Mjob_health',\n",
       " 'Fjob_health',\n",
       " 'Fjob_other',\n",
       " 'Fjob_services',\n",
       " 'reason_other',\n",
       " 'schoolsup_yes',\n",
       " 'famsup_yes',\n",
       " 'higher_yes']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a base classifier used to evaluate a subset of attributes\n",
    "estimatorSVR = SVR(kernel=\"linear\")\n",
    "# create the RFE model that incorporates k-fold cross validation where k = 5\n",
    "selectorSVR = RFECV(estimatorSVR, cv=5)\n",
    "# use the model on the training data to select the best features for prediction\n",
    "selectorSVR = selectorSVR.fit(predictors_train, target_train)\n",
    "# transform the training data to only include the selected features\n",
    "predictors_train_SVRselected = selectorSVR.transform(predictors_train)\n",
    "# transform the test data to only include the selected features\n",
    "predictors_test_SVRselected = selectorSVR.transform(predictors_test)\n",
    "# print out the features that were selected\n",
    "get_feature_names(selectorSVR, data, target_attribute_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR has chosen the above subset of 11 features as the best selection to train a predictive model. This is a lot less than the total number of features which is 41 (as explained in section 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Using Logistic Regression (LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['age',\n",
       " 'traveltime',\n",
       " 'studytime',\n",
       " 'failures',\n",
       " 'G1',\n",
       " 'G2',\n",
       " 'school_MS',\n",
       " 'reason_home',\n",
       " 'reason_other',\n",
       " 'higher_yes']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a base classifier used to evaluate a subset of attributes\n",
    "estimatorLR = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=6500)\n",
    "# create the RFE model that incorporates k-fold cross validation where k = 5\n",
    "selectorLR = RFECV(estimatorLR, cv=5)\n",
    "# use the model on the training data to select the best features for prediction\n",
    "selectorLR = selectorLR.fit(predictors_train, target_train)\n",
    "# transform the training data to only include the selected features\n",
    "predictors_train_LRselected = selectorLR.transform(predictors_train)\n",
    "# transform the test data to only include the selected features\n",
    "predictors_test_LRselected = selectorLR.transform(predictors_test)\n",
    "# print out the features that were selected\n",
    "get_feature_names(selectorLR, data, target_attribute_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR has chosen the above subset of 10 features as the best selection to train a predictive model.\n",
    "\n",
    "It is interesting to note the similarities between these features and the SVR selected features.\n",
    "\n",
    "- G1, G2, failures, reason_other and higher_yes are common to both.\n",
    "\n",
    "G1 and G2 will always be chosen as important features as these obviously have a high correlation with the target (G3)\n",
    "\n",
    "===================================================================================================================\n",
    "\n",
    "# 8 - Compare Classifiers\n",
    "\n",
    "We can use the cheat sheet from SK Learn, see below (note that this may not be visible if viewing in html, in which case the image can be found in the predictor_model folder), to help us pick a suitable algorithm for training a predictive model.\n",
    "\n",
    "Following the path that best describes our dataset, there are 4 algorithms suggested for us to try:\n",
    "\n",
    "- Linear SVC (section 8.1)\n",
    "- k-Nearest Neighbors (section 8.2)\n",
    "- SVC (section 8.3)\n",
    "- Ensemble Classifiers (Random Forest) (section 8.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ml_map.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each algorithm we will use __3 pipelines__ (one using features selected by SVR, one using features selected by LR and one using all features).\n",
    "\n",
    "The method we will use to train all our models is __Grid Search Cross Validation (GridSearchCV)__.\n",
    "\n",
    "One advantage of using GridSearchCV over cross_val_score, for example, is that it allows us to pass in a selection of hyper-parameters to the model.\n",
    "\n",
    "It will train a model using each available hyper-parameter and choose the best performing one.\n",
    "\n",
    "Examples of hyper-parameters used to tune our models include:\n",
    "- Penalty parameter of the error (__C__)\n",
    "- k-value for k-Nearest Neighbor (__n_neighbors__)\n",
    "- number of trees in Random Forest (__n_estimators__)\n",
    "- maximum depth of each Random Forest tree (__max_depth__)\n",
    "\n",
    "__Problem__: Because we are training 3 models (for each example of feature selection) for each of the 4 classifier algorithms (and using cross validation with 5 folds for each one), if we also pass in multiple different hyper-parameters for every single model we will end up with a large amount of computation required. The notebook would take a very long time to run all these models.\n",
    "\n",
    "__Solution__: So in order to reduce the work that this notebook will have to do, we decided to experiment with the hyper-parameters in seperate individual roughwork notebooks for each algorithm. We could then pick the best performing parameters and use them in this notebook (the best_params_ method for GridSearchCV allows us to do this).\n",
    "\n",
    "We ran each algorithm 5 times and picked the most frequent parameter values chosen. The results of our experimentation are as follows:\n",
    "\n",
    "- __C:__ Values Tested (0.01, 0.1, 1, 10, 100, 1000, 10000). __1000__ was chosen most frequently.\n",
    "\n",
    "- __n_neighbors:__ Values Tested (3, 5, 7, 9, 11). __7__ was chosen most frequently.\n",
    "\n",
    "- __n_estimators:__ Values Tested (100, 150, 200, 250, 300). __250__ was chosen most frequently.\n",
    "\n",
    "- __max_depth:__ Values Tested (2, 3, 4, 5). __4__ was chosen most frequently.\n",
    "\n",
    "These are the values that we will use for our 4 algorithms.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 8.1 Linear SVC\n",
    "\n",
    "#### a) Create classifer\n",
    "\n",
    "Support Vector Classifer (SVC) is type of Support Vector Machine (SVM) learning algorithm.\n",
    "\n",
    "Linear SVC is similar to SVC, but with the kernel parameter set to 'linear'.\n",
    "\n",
    "Linear SVC is generally not suited for a small dataset size. This is why we get the convergence warnings when we run it below. We tried increasing the maximum iterations as the warning suggest, but no matter how large a value we tried it did not solve the convergence warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note how we set the random_state to the constant seed value\n",
    "classifier = LinearSVC(random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Set up pipeline\n",
    "\n",
    "We pass in a scaler and the classifier. \n",
    "\n",
    "We could also incorporate feature selection in the pipeline itself, but we instead choose to only perform the feature selection once (in section 7) rather than repeating it for every pipeline. If we did incorporate it in the pipeline, the resulting features that would be selected would be identical everytime. Therefore this would be unneccessary and would require a lot of computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', MinMaxScaler()), ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Train models\n",
    "\n",
    "We will have 3 models for each algorithm:\n",
    "1. Using features selected by SVR\n",
    "2. Using features selected by LR\n",
    "3. Using all the features\n",
    "\n",
    "Note how we pass in a C value of 1000 as a hyper-parameter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.4s finished\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.3s finished\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.6s finished\n",
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# create a list of differant parameters that will be passed into the classifier in each pipeline\n",
    "params = {'classifier__C': [1000]}\n",
    "\n",
    "# train final models using the pipeline and params (grid search with cross-validation is used)\n",
    "modelSVR = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train_SVRselected, target_train)\n",
    "modelLR = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train_LRselected, target_train)\n",
    "modelAll = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: The warnings here relate to the fact that the model failed to converge, this is because Linear SVC does not work well with such a small dataset)\n",
    "\n",
    "#### d) Compare accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>SVR Features</th>\n",
       "      <th>LR Features</th>\n",
       "      <th>All Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classifier  SVR Features  LR Features  All Features\n",
       "0  Linear SVC          0.37         0.42          0.35"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_SVR = modelSVR.best_score_\n",
    "accuracy_LR = modelLR.best_score_\n",
    "accuracy_All = modelAll.best_score_\n",
    "\n",
    "scores = {'Classifier':['Linear SVC'],\n",
    "          'SVR Features':[accuracy_SVR],\n",
    "          'LR Features':[accuracy_LR],\n",
    "          'All Features':[accuracy_All]}\n",
    "\n",
    "# create dataframe to store all the accuracies\n",
    "accuracies_df = pd.DataFrame(scores)\n",
    "accuracies_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 8.2 kNN\n",
    "\n",
    "#### a) Create classifer\n",
    "\n",
    "In kNN, k is the number of nearest neighbors. The number of neighbors is the core deciding factor. \n",
    "\n",
    "It is a type of supervised ML algorithm which can be used for both classification as well as regression. However, it is mainly used for classification predictive problems in industry. The following two properties are associated with kNN:\n",
    "\n",
    "- Lazy learning. kNN is a lazy learning algorithm because it does not have a specialized training phase and uses all the data for training.\n",
    "\n",
    "- Non-parametric learning. kNN is also a non-parametric learning algorithm because it doesn’t assume anything about the underlying data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Set up pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', MinMaxScaler()), ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Train models\n",
    "\n",
    "Note how we pass in a n_neighbors value of 7 as a hyper-parameter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "params = {'classifier__n_neighbors': [7]}\n",
    "\n",
    "# train final models using the pipeline and params (grid search with cross-validation is used)\n",
    "modelSVR = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train_SVRselected, target_train)\n",
    "modelLR = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train_LRselected, target_train)\n",
    "modelAll = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Compare accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>SVR Features</th>\n",
       "      <th>LR Features</th>\n",
       "      <th>All Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>kNN</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classifier  SVR Features  LR Features  All Features\n",
       "0  Linear SVC          0.37         0.42          0.35\n",
       "1         kNN          0.57         0.51          0.30"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_SVR = modelSVR.best_score_\n",
    "accuracy_LR = modelLR.best_score_\n",
    "accuracy_All = modelAll.best_score_\n",
    "\n",
    "# add accuracies to dataframe\n",
    "accuracies_df.loc[1] = ['kNN', accuracy_SVR, accuracy_LR, accuracy_All]\n",
    "accuracies_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 8.3 SVC\n",
    "\n",
    "#### a) Create classifer\n",
    "\n",
    "Support Vector Classifier (SVC), as the name suggests, is similar to Linear SVC. However, it is a more accurate algorithm than Linear SVC.\n",
    "\n",
    "SVC is better suited to a small dataset than Linear SVC. This is why we do not get any convergance warnings when we run this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note how we set the random_state to the constant seed value\n",
    "classifier = svm.SVC(gamma='scale', random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Set up pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', MinMaxScaler()), ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Train models\n",
    "\n",
    "Note how we pass in a C value of 1000 as a hyper-parameter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "params = {'classifier__C': [1000]}\n",
    "\n",
    "# train final models using the pipeline and params (grid search with cross-validation is used)\n",
    "modelSVR = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train_SVRselected, target_train)\n",
    "modelLR = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train_LRselected, target_train)\n",
    "modelAll = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Compare accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>SVR Features</th>\n",
       "      <th>LR Features</th>\n",
       "      <th>All Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>kNN</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classifier  SVR Features  LR Features  All Features\n",
       "0  Linear SVC          0.37         0.42          0.35\n",
       "1         kNN          0.57         0.51          0.30\n",
       "2         SVC          0.67         0.62          0.46"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_SVR = modelSVR.best_score_\n",
    "accuracy_LR = modelLR.best_score_\n",
    "accuracy_All = modelAll.best_score_\n",
    "\n",
    "# add accuracies to dataframe\n",
    "accuracies_df.loc[2] = ['SVC', accuracy_SVR, accuracy_LR, accuracy_All]\n",
    "accuracies_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "## 8.4 Ensemble Classifiers (Random Forest)\n",
    "\n",
    "#### a) Create classifer\n",
    "\n",
    "There are two main types of Ensemble Classifiers:\n",
    "1. Random Forest\n",
    "2. Bagging\n",
    "\n",
    "We have chosen Random Forest for our dataset as we are more familier with algorithm from experimentation in our labs.\n",
    "\n",
    "Random Forest is a learning method that operates by constructing multiple decision trees. The final decision is made based on the majority of the trees and is chosen by the Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note how we set the random_state to the constant seed value\n",
    "classifier = RandomForestClassifier(random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Set up pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('scaler', MinMaxScaler()), ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Train models\n",
    "\n",
    "Note how we pass in an n_estimators value of 250 and a max_depth value of 4 as hyper-parameters here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.8s finished\n"
     ]
    }
   ],
   "source": [
    "params = {'classifier__n_estimators': [250],\n",
    "          'classifier__max_depth': [4]}\n",
    "\n",
    "# train final models using the pipeline and params (grid search with cross-validation is used)\n",
    "modelSVR = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train_SVRselected, target_train)\n",
    "modelLR = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train_LRselected, target_train)\n",
    "modelAll = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Compare accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classifier</th>\n",
       "      <th>SVR Features</th>\n",
       "      <th>LR Features</th>\n",
       "      <th>All Features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>kNN</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>SVC</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Ensemble (RF)</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Classifier  SVR Features  LR Features  All Features\n",
       "0     Linear SVC          0.37         0.42          0.35\n",
       "1            kNN          0.57         0.51          0.30\n",
       "2            SVC          0.67         0.62          0.46\n",
       "3  Ensemble (RF)          0.71         0.72          0.65"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_SVR = modelSVR.best_score_\n",
    "accuracy_LR = modelLR.best_score_\n",
    "accuracy_All = modelAll.best_score_\n",
    "\n",
    "# add accuracies to dataframe\n",
    "accuracies_df.loc[3] = ['Ensemble (RF)', accuracy_SVR, accuracy_LR, accuracy_All]\n",
    "accuracies_df.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Observations__\n",
    "\n",
    "- There is relatively little difference between using the features selected by SVR or by LR.\n",
    "- Using all the features has consistently poor results compared with selected features.\n",
    "- The results reflect the SK Learn Algorithm Cheat-Sheet.\n",
    "- A __Random Forest__ classifier with __features selected by Logistic Regression__ has the best performance.\n",
    "\n",
    "===================================================================================================================\n",
    "\n",
    "# 9 - Final Predictive Model\n",
    "\n",
    "The above results prove that a __Random Forest classifier__ with __features selected by Logistic Regression__ is the best performing model.\n",
    "\n",
    "We can now get a final accuracy using this model on the __test data__.\n",
    "\n",
    "We will setup the model again just to ensure we have the correct variables selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    1.8s finished\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(random_state=seed)\n",
    "pipe = Pipeline([('scaler', MinMaxScaler()), ('classifier', classifier)])\n",
    "params = {'classifier__n_estimators': [250],\n",
    "          'classifier__max_depth': [4]}\n",
    "modelFinal = GridSearchCV(pipe, params, cv=5, verbose=1, iid=False).fit(predictors_train_LRselected, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the final accuracy using the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our final predictive model is: 72%\n"
     ]
    }
   ],
   "source": [
    "accuracy_Final = modelFinal.score(predictors_test_LRselected, target_test)\n",
    "print(\"The accuracy of our final predictive model is: %2d%%\" %(accuracy_Final.round(2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    35\n",
       "4    31\n",
       "2    28\n",
       "1    17\n",
       "0     9\n",
       "5     6\n",
       "Name: grade, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===================================================================================================================\n",
    "\n",
    "# 10 - Evaluation of Final Model\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "This confusion matrix is based on the test dataset (20% of overall data, 126 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_confusion_matrix_heatmap(target_test, modelFinal.best_estimator_.predict(predictors_test_LRselected))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the confusion matrix that the results are relatively good by the high values on the diagonal.\n",
    "- It is interesting to note that the model did not make any predictions for grade 'F'. It instead wrongly predicted the 6 true 'F' grades as 'E' grades.\n",
    "- It predicted more 'A' grades wrong (5 as 'B') than right (4 as 'A').\n",
    "- All 17 true 'B' grades were predicted correctly.\n",
    "- The majority of 'C' grades were predicted correctly (20 out of 28).\n",
    "- The majority of 'D' grades were predicted correctly (21 out of 35).\n",
    "- The majority of 'E' grades were predicted correctly (29 out of 31)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision-Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.44      0.62         9\n",
      "           1       0.68      1.00      0.81        17\n",
      "           2       0.77      0.71      0.74        28\n",
      "           3       0.75      0.60      0.67        35\n",
      "           4       0.67      0.94      0.78        31\n",
      "           5       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.72       126\n",
      "   macro avg       0.65      0.62      0.60       126\n",
      "weighted avg       0.71      0.72      0.70       126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/endamccarthy/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_test, modelFinal.best_estimator_.predict(predictors_test_LRselected)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: The warning here is present because the was no prediction for an 'F' grade.)\n",
    "\n",
    "We can see how the Precision-Recall report backs up the Confusion Matrix:\n",
    "\n",
    "The __precision__ of an 'A' grade (0) prediction is 100%. This means that every time the model predicted a grade of 'A', it was correct. This can be seen in the Confusion Matrix where there are 4 predicted 'A' grades which are all correct.\n",
    "\n",
    "The __precision__ of a 'B' grade (1) prediction is 68%. This means that every time the model predicted a grade of 'B', it was correct only 68% of the time. This is supported by the Confusion Matrix where only 17 out of 25 (68%) predicted 'B' grades were correct.\n",
    "\n",
    "The __recall__ of an 'A' grade (0) prediction is 44%. This means that out of all the true 'A' grades, only 44% were predicted correctly. This is supported by the Confusion Matrix where only 4 out of 9 (44%) true 'A' grades were predicted correctly.\n",
    "\n",
    "The __recall__ of a 'B' grade (1) prediction is 100%. This means that out of all the true 'B' grades, 100% were predicted correctly. This is supported by the Confusion Matrix where 17 out of 17 (100%) true 'B' grades were predicted correctly.\n",
    "\n",
    "The __overall accuracy__ is 72% which we already highlighted in section 9.\n",
    "\n",
    "The __f1-score__ is given by the harmonic mean of precision and recall, it combines precision and recall of a class in one metric:\n",
    "    \n",
    "    (2 × precision × recall / (precision + recall)) \n",
    "    \n",
    "===================================================================================================================\n",
    "\n",
    "# 11 - Conclusion\n",
    "\n",
    "\n",
    "\n",
    "Having followed all the steps from the labs 2-5, we have successfully practiced EDA, data preparation, train/test splits, feature selection, comparison of predictive models and evaluation of the best performing model throughout this notebook and the EDA notebook. We will demonstrate lab 6 (clustering) in another seperate notebook.\n",
    "\n",
    "These are some of the observations and learning outcomes we made along the way:\n",
    "\n",
    "- It was beneficial to reduce the target from 20 values (1-20) into 6 (A-F) categories. It is obviously easier to predict a value from 6 options rather than from 20 options. Our accuracy (72%) is still not quite high enough for this model to be considered a success. It would be interesting to reduce the categories again to 5 or 4 options. This would certainly improve the overall accuracy.\n",
    "\n",
    "- Even after reducing the target into 6 categories there was still a certain amount of imbalance present (much more 'C', 'D' and 'E' grades than 'A' and 'F') which may have affected our model results. An interesting experiment to try when imbalance is present is to perform resampling on the dataset. However, this can have a negative impact if not fully understood and implemented correctly. \n",
    "\n",
    "- We used a train/test split of 20% which is generally recommended. If we had chosen a larger test split (say 30%), we may have ended up with greater variance in our predictions, but lower bias. There is a trade off between these, so 20% is generally recommended as a good balance.\n",
    "\n",
    "- It is interesting to note how much longer it took to perform feature selection using Logistic Regression than SVR (roughly 10 times as long). We researched this but could not find any conclusive explanations.\n",
    "\n",
    "Finally, we conclude that for the dataset used for this project, the Logistic Regression resulted as the best feature selector and Random Forest as the best classifier in our prediction model for G3.\n",
    "\n",
    "-------------------------------------------------------------------------------------------------------------------\n",
    "__Links__\n",
    "- [SK Learn Cheat-Sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)\n",
    "- [Linear SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC)\n",
    "- [k-Nearest Neighbors](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier)\n",
    "- [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC)\n",
    "- [Ensemble](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n",
    "- [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)\n",
    "\n",
    "==================================================================================================================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
